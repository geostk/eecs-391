# Find the third lowest mse and compare to current mse
third_lowest_mse <- sort(all_mse, decreasing = TRUE)[n_mse - 2]
if (mse > third_lowest_mse) {
print('MSE Inceasing.')
return(results_df)
}
}
weights <- as.numeric(weights - learning_rate * gradient)
# Function takes in the iris data set, initial weights, number of iterations,
# learning rate, and tolerance
# Return is a dataframe that can be used for plotting
# the decision boundary and learning curve
grad_descent_complete <- function(iris_data, initial_weights, n_steps = 5,
learning_rate = 0.01, tolerance = 0.005) {
# Add intercept to data and determine n
iris_data$intercept <- 1
n_obs <- nrow(iris_data)
# Enforce column order for features and create label vector
data <- dplyr::select(iris_data, intercept, petal_length, petal_width)
labels <- ifelse(iris_data$species == 'versicolor', 0, 1)
weights <- initial_weights
data <- data.matrix(data)
# Variables to hold metrics
results_df <- as.data.frame(matrix(ncol = 5, nrow = n_steps))
names(results_df) <- c('iteration', 'mse', 'w0', 'w1', 'w2')
# Iterate for the specified number of steps
for (i in 1:n_steps) {
# Make predictions and calculate errors
predictions <- sapply(data %*% weights, sigmoid)
errors <- predictions - labels
# Record mean squared error
mse <- mean((errors) ^ 2)
# Calculate the gradient
gradient <- (2 / n_obs) * matrix((1 - predictions) * (errors), ncol = n_obs) %*% data
# Record results for plotting
results_df[i, 'iteration'] = i
results_df[i, 'mse'] = mse
results_df[i, 'w0'] = weights[1]
results_df[i, 'w1'] = weights[2]
results_df[i, 'w2'] = weights[3]
# First stopping criteria
if (sqrt(sum(gradient ^ 2)) < tolerance) {
print("Minimum Tolerance Reached.")
return(results_df)
}
# Second stopping criteria
all_mse <- results_df$mse[!is.na(results_df$mse)]
if (length(all_mse) > 3) {
n_mse <- length(all_mse)
# Find the third lowest mse and compare to current mse
third_lowest_mse <- sort(all_mse, decreasing = TRUE)[n_mse - 2]
if (mse > third_lowest_mse) {
print('MSE Inceasing.')
return(results_df)
}
}
# Update the weights
weights <- as.numeric(weights - learning_rate * gradient)
# Display progress every 50 iterations
if (i %% 50 == 0) {print(sprintf('Iteration: %0.0f, mse: %0.4f', i, mse))
}
}
# Max iterations reached
print('Maximum number of iterations reached.')
return(results_df)
}
# Function to plot the decision boundary and the learning curve
plot_decision_learning <- function(results, iris_data) {
# Plot results from beginning, middle, and end of iterations
rows <- seq(1, nrow(results), by = nrow(results)/2 - 1)
for  (i in rows) {
# Select the data for plotting
data_row <- results[i, ]
weights <- c(data_row$w0, data_row$w1, data_row$w2)
i <- data_row$iteration
mse <- data_row$mse
# Plot decision boudary
print(ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
geom_point() +
geom_abline(slope = -weights[2]/weights[3],
intercept = -weights[1]/weights[3],
color = 'blue', lwd = 1.1) +
xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle(sprintf('Iteration: %0.0f MSE: %0.4f    w0: %0.2f, w1: %0.2f, w2: %0.2f',
i, mse, weights[1], weights[2], weights[3])) +
theme_classic(12) + coord_cartesian(xlim = c(2.5, 8), ylim = c(1.0, 4.0)) +
scale_color_manual(values = c('firebrick', 'darkgreen')))
learning_data <- results[1:i, ]
print(ggplot(learning_data, aes(x = iteration, y = mse)) +
geom_point(color = 'firebrick', shape = 4) +  ggtitle("Learning curve"))
}
}
# Start with a high decision boundary
results_high <- grad_descent_complete(iris_subset, initial_weights = c(-6, 0.4, 1.2),
n_steps = 500, learning_rate = 0.005, tolerance = 0.005)
# Start with a low decision boundary
results_low <- grad_descent_complete(iris_subset, initial_weights = c(-3.6, 0.6, 1.2),
n_steps = 500, learning_rate = 0.005, tolerance = 0.005)
# Plot results for first run
results_high <- results_high[complete.cases(results_high), ]
plot_decision_learning(results_high, iris_subset)
# Plot results for second run
results_low <- results_low[complete.cases(results_low), ]
plot_decision_learning(results_low, iris_subset)
# Plot results for first run
results_high <- results_high[complete.cases(results_high), ]
plot_decision_learning(results_high, iris_subset)
# Plot results for second run
results_low <- results_low[complete.cases(results_low), ]
plot_decision_learning(results_low, iris_subset)
y <- seq(0, 4, 1)
prob <- function(y) {
first_term <- (factorial(4) / (factorial((4 - y)) * factorial(y)))
second_term <- (0.75 ^ y) * (0.25 ^ (4 - y))
return(first_term * second_term)
}
prob_y <- sapply(y, prob)
y_smooth <- seq(0, 4, by = 4/100)
prob_y_smooth <- sapply(y_smooth, prob)
plot(y, prob_y, main = 'Likelihood of y') + lines(y_smooth, prob_y_smooth) +
text(y, prob_y - 0.04, labels = round(prob_y, 2))
calc_posterior <- function(theta, y, n) {
first_term <- factorial(n + 1) / (factorial(n - y) * factorial(y))
second_term <- (theta ^ y) * ((1 - theta) ^ (n - y))
return(first_term * second_term)
}
y <- c(1, 2, 2, 3)
n <- c(1, 2, 3, 4)
theta <- seq(0, 1, 1/1000)
calc_posterior <- function(theta, y, n) {
first_term <- factorial(n + 1) / (factorial(n - y) * factorial(y))
second_term <- (theta ^ y) * ((1 - theta) ^ (n - y))
return(first_term * second_term)
}
y <- c(1, 2, 2, 3)
n <- c(1, 2, 3, 4)
theta <- seq(0, 1, 1/1000)
posterior_df <- as.data.frame(matrix(nrow = length(theta) * 4, ncol = 4))
names(posterior_df) <- c('n', 'y', 'theta', 'posterior')
i <- 1
all_posteriors <- c()
for (head_count in y) {
posteriors <- sapply(theta, calc_posterior, y = head_count, n = i)
all_posteriors <- c(all_posteriors, posteriors)
i <- i + 1
}
posterior_df$n <- c(rep(1, 1001), rep(2, 1001),
rep(3, 1001), rep(4, 1001))
posterior_df$y <- c(rep(1, 1001), rep(2, 2002),
rep(3, 1001))
posterior_df$n_y <- c(rep(c('n = 1, y = 1'), 1001), rep(c('n = 2, y = 2'), 1001),
rep(c('n = 3, y = 2'), 1001), rep(c('n = 4, y = 3'), 1001))
posterior_df$theta <- c(rep(theta, 4))
posterior_df$posterior <- all_posteriors
ggplot(posterior_df, aes(theta, posterior)) + geom_point(col = 'darkgreen') +
geom_line(col = 'darkgreen') +
facet_wrap(~n_y) + ylab('Posterior for theta') +
ggtitle('Posterior Distributions for theta') + theme_solarized_2() +
theme(panel.grid = element_blank(), text =  element_text(face = 'bold', color = 'black'),
title = element_text(face = 'bold', color = 'black'))
priors <- list('h1' = 0.1, 'h2' = 0.2, 'h3' = 0.4, 'h4' = 0.2, 'h5' = 0.1)
hypotheses <- list('h1' = 0.0, 'h2' = 0.25, 'h3' = 0.5, 'h4' = 0.75, 'h5' = 1.0)
calc_likelihood <- function(theta, y, n) {
first_term <- factorial(n) / (factorial(n - y) * factorial(y))
second_term <- (theta ^ y) * ((1 - theta) ^ (n - y))
return(first_term * second_term)
}
calc_posteriors <- function(true_hyp) {
# Set seed to ensure consistent results
set.seed(42)
# Create a dataframe to hold results
posts_df <- as.data.frame(matrix(ncol = 8))
names(posts_df) <- c('n', 'y', 'h1', 'h2', 'h3', 'h4', 'h5', 'next_lime')
# Vector of observations under the true hypothesis
limes <- c(0, rbinom(n = 99, size = 1, prob = true_hyp))
total_limes <- cumsum(limes)
# Priors for each hypothesis and the conditional probability of a lime candy
# for each hypothesis
priors <- list('h1' = 0.1, 'h2' = 0.2, 'h3' = 0.4, 'h4' = 0.2, 'h5' = 0.1)
lime_pct <- list('h1' = 0.0, 'h2' = 0.25, 'h3' = 0.5, 'h4' = 0.75, 'h5' = 1.0)
# Iterate through the observations and calculate posteriors and
# chance of next lime
for (i in seq(1, 100, 1)) {
n_lime <- total_limes[i]
# Calculate the unnormalized posterior for each hypothesis
h1_post <- calc_likelihood(lime_pct$h1, n_lime, i - 1) * priors$h1
h2_post <- calc_likelihood(lime_pct$h2, n_lime, i - 1) * priors$h2
h3_post <- calc_likelihood(lime_pct$h3, n_lime, i - 1) * priors$h3
h4_post <- calc_likelihood(lime_pct$h4, n_lime, i - 1) * priors$h4
h5_post <- calc_likelihood(lime_pct$h5, n_lime, i - 1) * priors$h5
# Calculate the normalization constant and normalize the posteriors
posts <- c(h1_post, h2_post, h3_post, h4_post, h5_post)
norm_constant <- sum(posts)
norm_posts <- posts / norm_constant
# The probability of a lime next
next_lime <- sum(unlist(lime_pct) * norm_posts)
# Explicitly add results to a dataframe
posts_df <- add_row(posts_df, n = i - 1, y = n_lime, h1 = norm_posts[1], h2 = norm_posts[2],
h3 = norm_posts[3], h4 = norm_posts[4], h5 = norm_posts[5],
next_lime = next_lime)
}
return(posts_df[-1, ])
}
h1_true <- calc_posteriors(0.0)
h2_true <- calc_posteriors(0.25)
h3_true <- calc_posteriors(0.5)
h4_true <- calc_posteriors(0.75)
h5_true <- calc_posteriors(1.0)
graph_posts <- function(post_df, true_hyp_str) {
# Titles for figures
main1 <- sprintf('Posteriors under %s', true_hyp_str)
main2 <- sprintf('Probability of next lime under %s', true_hyp_str)
# Put results in long format with each row and observation and each column
# a variable
long_post <- gather(post_df, key = 'hyp', value = 'post', -n, -y, -next_lime)
color_vector <- c('blue', 'red', 'darkgreen', 'darkorange', 'black')
th  <- theme(axis.text = element_text(color = 'black'))
# Graph the posteriors for each hypothesis under the true hypothesis
p1 <- ggplot(long_post, aes(n, post, col = hyp, group = hyp, shape = hyp)) + geom_line() +
geom_point() + xlab('Number of Observations') + ylab('Posterior') +
ggtitle(main1) + theme_economist(12) +  th + scale_color_manual(values = color_vector)
# Graph the probability of the next candy being a lime
p2 <- ggplot(post_df, aes(n, next_lime)) + geom_line(col = 'red') + geom_point(col = 'red') +
xlab('Number of Observations') + ylab('Probability') + ggtitle(main2) +
coord_cartesian(ylim = c(0, 1)) + scale_y_continuous(breaks = seq(0, 1, 0.1)) +
theme_economist(12) + th
# Display the graphs
print(p1)
print(p2)
}
graph_posts(h1_true, 'h1')
graph_posts(h2_true, 'h2')
graph_posts(h3_true, 'h3')
graph_posts(h4_true, 'h4')
graph_posts(h5_true, 'h5')
iris_data <- read_csv('irisdata.csv')
kmeans <- function(k, iris_df, max_iter = 5) {
# Select the features used for clustering
iris_df <- dplyr::select(iris_df, petal_length, petal_width, species)
# Choose initial cluster centers at random from all data points
initial_indices <- sample(1:length(iris_df$species), size = k)
cluster_centers <- data.matrix((iris_df[initial_indices, c(1,2)]))
# Dataframe to track progression of cluster centers
track_centers <- as.data.frame(matrix(ncol = 2))
names(track_centers) <- c('petal_length', 'petal_width')
# Used for keeping track of iterations
index <- c()
center_index <- c()
# Vector with all errors
total_errors <- c()
# Iterate for the max iterations or until convergence
for (n in 1:max_iter) {
# Needed for convergence check
previous_cluster_centers <- cluster_centers
# Holds the class assignments
classes <- c()
# Track progression of clusters
track_centers <- rbind(track_centers, cluster_centers)
# Iterate through the irises
for (i in 1:nrow(iris_df)) {
# Select an iris
iris <- as.numeric((dplyr::select(
iris_df, petal_length, petal_width))[i, ])
distances <- c()
# Iterate through the cluster centers
for (j in 1:k) {
center <- cluster_centers[j, ]
# Calculate the Euclidean distance between each point
# and the cluster center
distance <- dist(matrix(data = c(center, iris),
ncol = 2, byrow = TRUE),
method = 'euclidean')
distances <- c(distances, distance)
}
# The class is the cluster center with the
# minimum distance from the iris
class <- which.min(distances)
classes <- c(classes, class)
}
# Assign classes to all the irises (irisi perhaps?)
iris_df$class <- classes
# Used to record errors for assignment of cluster centers
track_errors <- c()
# Loop to update all the cluster centers
for (class_num in unique(iris_df$class)) {
# Extract only those points assigned to the cluster
class_df <- dplyr::filter(iris_df, class == class_num)
# Calculate error associated with cluster center
class_center <- cluster_centers[class_num, ]
# Error associated with each feature
class_df$length_error <- class_df$petal_length - class_center[1]
class_df$width_error <- class_df$petal_width - class_center[2]
# Total error is the sum of the individual feature squared errors
class_df <- dplyr::mutate(class_df, total_error =
length_error ^ 2 + width_error ^ 2)
# Keep track of the errors
total_cluster_error <- sum(class_df$total_error)
track_errors <- c(track_errors, total_cluster_error)
# Update rule for cluster center
cluster_centers[class_num, ] = c(mean(class_df$petal_length),
mean(class_df$petal_width))
}
# The total error is the sum of the errors for each cluster
total_errors <- c(total_errors, sum(track_errors))
# Used for tracking the changes in cluster centers
index <- c(index, rep(n, k))
center_index <- c(center_index, seq(1, k, by = 1))
# Convergence condition
if (all(previous_cluster_centers == cluster_centers)) {
print('Convergence Achieved')
track_centers <- track_centers[complete.cases(track_centers), ]
track_centers$iter <- index
track_centers$center <- center_index
# Return the cluster centers and errors for plotting
return(list('cluster_centers' = track_centers,
'errors' = total_errors))
# If convergence not achieved continue iteration
} else {
previous_cluster_centers <- cluster_centers
}
# Provide feedback
print(sprintf('Iteration: %0.0f total error: %0.2f', n,
sum(track_errors)))
}
# Return the position of clusters over iterations
track_centers <- track_centers[complete.cases(track_centers), ]
track_centers$iter <- index
track_centers$center <- center_index
print(sprintf('Max iterations: %0.0f reached with error: %0.4f',
max_iter, sum(track_errors)))
return(list('cluster_centers' = track_centers, 'errors' = total_errors))
}
results <- kmeans(3, iris_data, max_iter = 5)
cluster_df <- results$cluster_centers
total_error <- results$errors
View(cluster_df)
seq(1, nrow(cluster_df), by = (nrow(cluster_df)/2) - 1))
seq(1, nrow(cluster_df), by = (nrow(cluster_df)/2) - 1)
iterations <- unique(cluster_df$iter)
seq(1, max(iterations), by = (length(floor(cluster_df/2))))
seq(1, max(iterations), by = (length(floor(cluster_df/2))) - 1)
seq(1, max(iterations), by = floor(length(cluster_df/2)) - 1)
seq(1, max(iterations), by = floor(length(cluster_df)/2) - 1)
seq(1, max(iterations), by = floor(length(cluster_df)/2))
results <- kmeans(3, iris_data, max_iter = 6)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
iter = 1
iteration = 1
cluster_subset <- dplyr::select(cluster_df, iter == iteration)
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = as.factor(center)),
color = 'blue', size = 2)
ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = as.factor(center)),
color = 'orange', size = 4)
ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = as.factor(center)),
color = 'orange', size = 4) + labs(color = 'center')
cluster_subset$center <- as.factor(cluster_subset$center)
results <- kmeans(3, iris_data, max_iter = 6)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
}
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
p <- ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
print(p)
}
results <- kmeans(2, iris_data, max_iter = 6)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
p <- ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
print(p)
}
results <- kmeans(2, iris_data, max_iter = 12)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
p <- ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
print(p)
}
results <- kmeans(3, iris_data, max_iter = 12)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
p <- ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
print(p)
}
results <- kmeans(3, iris_data, max_iter = 12)
results <- kmeans(3, iris_data, max_iter = 12)
cluster_df <- results$cluster_centers
total_error <- results$errors
iterations <- unique(cluster_df$iter)
for (iteration in seq(1, max(iterations),
by = floor(length(cluster_df)/2))) {
# Subset the data to the relevent iteration
cluster_subset <- dplyr::filter(cluster_df, iter == iteration)
cluster_subset$center <- as.factor(cluster_subset$center)
# Plot the iris data and the cluster centers
p <- ggplot(iris_data, aes(x = petal_length, y = petal_width,
color = species)) +
geom_point() + xlab('Petal Length (cm)') +
ylab('Petal Width (cm)') +
ggtitle('Cluster Centers with Iris Data') + theme_classic(12) +
scale_color_manual(values = c('firebrick', 'darkgreen', 'navy')) +
geom_point(data = cluster_subset, aes(x = petal_length, y = petal_width,
shape = center),
color = 'orange', size = 4) +
ggtitle(sprintf('Iteration %0.0f, Error: %0.4f', iteration,
total_error[iteration]))
print(p)
}
