---
title: "EECS 391 Programming Assignment Two"
author: "Will Koehrsen"
date: "December 9, 2017"
output:
  pdf_document:
    toc: true
    number_sections: true
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root_dir = 'C:/Users/Will Koehrsen/Documents/eecs-391/Programming/Project2')

library(tidyverse)
library(ggthemes)
library(ggforce)
```

# Linear Decision Boundaries

## A. Inspection and Plotting

The first step is to inspect the iris data set. I can load in the data into a dataframe
and perform some simple explorations. 

```{r}
# Load in data into a dataframe
iris <- read_csv('irisdata.csv')

# Structure of the data
str(iris, give.attr = FALSE)
```
We can see there are 150 observations of 5 different variables. The variables are sepal length,
sepal width, petal length, petal width, and species. For the iris data set, the four numerical
measurements are the features and the 
species is the label. 

```{r}
table(iris$species)
```
There are three species, each with 50 observations. 

Next, we can plot the variables to see the ranges within each species. We are asked to plot
only the second and third classes, veriscolor and virginica. 

```{r}
# Create a subset with only the relevant classes
iris_subset <- dplyr::filter(iris, species == 'versicolor' | species == 'virginica')

# Plot the features colored by class
ggplot(iris_subset, aes(x = sepal_length, y = sepal_width, color = species)) + 
  geom_jitter() + xlab('Sepal Length (cm)') + 
  ylab('Sepal Width (cm)') + 
  ggtitle('Sepal Width vs Length by Iris Species') + theme_classic(12) + 
  scale_color_manual(values = c('firebrick', 'darkgreen'))

ggplot(iris_subset, aes(x = petal_length, y = petal_width, color = species)) + 
  geom_jitter() + xlab('Petal Length (cm)') + 
  ylab('Petal Width (cm)') + 
  ggtitle('Petal Width vs Length by Iris Species') + theme_classic(12) + 
  scale_color_manual(values = c('firebrick', 'darkgreen'))

```
We can see that there is clearly a line that can separate the two classes
on the plot of Petal Length vs Width. A simple linear decision boundary
should have decent accuracy in separating the classes. 

## B. Plot A Linear Decision Boundary

The following function plots a linear decision boundary over the data. 
The slope and intercept are chosen by inspecting the data and trying to choose a 
line to minimize the classification error. 

```{r}
# Function to plot a linear decision boundary given slope and intercept
plot_db <- function(iris_data, m, b) {
  ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) + 
    geom_jitter() + geom_abline(slope = m, intercept = b, color = 'blue', lwd = 1.1) + 
    xlab('Petal Length (cm)') + 
  ylab('Petal Width (cm)') + 
  ggtitle('Petal Width vs Length by Iris Species') + theme_classic(12) + 
    scale_color_manual(values = c('firebrick', 'darkgreen'))
}

m <- -0.5
b <- 4
# Plot a selected boundary
plot_db(iris_subset, m, b)

```

## C. Define a Simple Threshold Classifier Using the above Decision Boundary

The following function takes in a petal length and a petal width and returns 
the class (species) of the iris. The threshold classifier mutliplies the given 
weights by the observation features, and classifies based on a threshold of
zero. The weights hav eben selected to correspond to the decision boundary
plotted above. 

```{r}
# Function takes in a petal length, petal width, and iris data set
# and makes a class prediction. The function also plots the 
# data and the new observation by default
classify <- function(petal_length, petal_width, iris_data, 
                     plot_results = TRUE, w0 = -4, w1 = 0.5, w2 = 1) {
  
  if ( (petal_length * w1 + petal_width * w2 + w0) < 0) {
    class = 'versicolor'
    
  } else if ((petal_length * w1 + petal_width * w2 + w0) >= 0) {
    class = 'virginica'
    
  } else {
    class = sample(c('versicolor', 'virgnica'), size = 1)
  }
  
  
  iris_data$set <- 'train'
  
  iris_data <- add_row(iris_data, petal_width = petal_width, petal_length = petal_length,
                       sepal_width = NA, sepal_length = NA, species = class, set = 'prediction')
  if (plot_results) {
  print(ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species, shape = set)) + 
    geom_jitter() + 
      geom_abline(slope = -w1/w2, intercept = -w0/w2, color = 'blue', lwd = 1.1) + 
    xlab('Petal Length (cm)') + 
  ylab('Petal Width (cm)') + 
  ggtitle('Petal Width vs Length by Iris Species') + theme_classic(12) + 
      scale_color_manual(values = c('firebrick', 'darkgreen')))
  }
    
  print(sprintf('The class prediction is: %s', class))
}

# Example of new data point
classify(petal_length = 2.5, petal_width = 5, iris_data = iris_subset) 

# Another example of new data point
classify(petal_length = 2.5, petal_width = 2.5, iris_data = iris_subset) 
```
To demonstrate the classification function, I will plot a few of the actual points 
in the data. 

```{r}
# Plot a known versicolor example
iris_subset$species[6]
petal_length <- iris_subset$petal_length[6]
petal_width <- iris_subset$petal_width[6]

classify(petal_length, petal_width, iris_subset)


# Plot a known virginica example
iris_subset$species[66]
petal_length <- iris_subset$petal_length[66]
petal_width <- iris_subset$petal_width[66]

classify(petal_length, petal_width, iris_subset)
```

The linear classifier correctly identifies both classes. There are several instances near the 
boundary the model mis-classifies because the decision boundary is not entirely linear.

## D. Define a circle decision boundary using a single point as center

The function below takes a center for the circle and a radius and draws a 
circular decision boundary. Points are classifed using the Euclidean distance.
If points are within the raidus from the center of the circle in terms of
the Euclidean distance, then they are within the circle. The class for points within the 
circle is determined by the iris class with the most points in the cirlce. 
Accuracy is assessed as the number of points correctly classified divided by the 
total number of iris observations. 

```{r}
# Takes a circle center (c(x, y)) and a circle radius and predicts the classes
# of the iris_data. Prints the accuracy of the circular classifier
# and displays a plot with the classifier and original data.
circular_db <- function(center, radius, iris_data) { 
  classes <- c()
  
  # Make a classification for each pointbased on the circle parameters
  for (i in 1:nrow(iris_data)) {
    length <- iris_data[[i, 'petal_length']]
    width <- iris_data[[i, 'petal_width']]
    
    # Check if point in circle using Euclidean distance
    in_circle <- ifelse(dist(matrix(c(center[1], center[2], length, width), 
                                    nrow = 2, ncol = 2, byrow = TRUE)) < radius, 1, 0)
    classes <- c(classes, in_circle)
  }
  
  iris_data$in_circle <- classes
  

  virginica_in <- sum(iris_data$in_circle == 1 & 
                        iris_data$species == 'virginica')
  
  versicolor_in <- sum(iris_data$in_circle == 1 & 
                         iris_data$species == 'versicolor')
  
  # Set in circle class to class with more points in circle
  # If tied for number, choose a random class
  # The tie also handles the cases where no points 
  # are in circle and all points are in circle
  circle_class <- ifelse(virginica_in > versicolor_in, 'virginica', 
                         ifelse(virginica_in < versicolor_in, 'versicolor', 
                                sample(c('virginica', 'versicolor'), 1)))
  
  # The non-circle class is the other class
  non_circle_class <- ifelse(circle_class == 'virginica', 'versicolor', 'virginica')
  
  # Prediction is the label of in circle or not 
  iris_data$prediction <- ifelse(iris_data$in_circle == 1, circle_class, non_circle_class)
  
  # Accuracy is the average of correct predictions
  accuracy <- sum(iris_data$prediction == iris_data$species) / nrow(iris_data)

  # Create a dataframe for plotting the circle
  circ_df <- data.frame(x = center[1], y = center[2], radius = radius)
  
  # Plot the data and the circular decision boundary
  p <- ggplot(iris_subset) + geom_jitter(aes(x = petal_length, y = petal_width, color = species)) + 
    geom_circle(data = circ_df, aes(x0 = x, y0 = y, r = radius)) + coord_fixed() +
    xlab('Petal Length (cm)') + 
  ylab('Petal Width (cm)') + 
  ggtitle('Petal Width vs Length by Iris Species with Circular DB') + theme_classic(12) + 
    scale_color_manual(values = c('firebrick', 'darkgreen'))
  
  print(p) 
  
 print(sprintf('Accuracy: %0.2f%% with circle at %0.2f, %0.2f with radius %0.2f.',
               accuracy * 100, center[1], center[2], radius))
  }

```

The next step is to test the classifcation accuracy of the circle with several
values for the center and radius.

```{r}
# Test classification accuracy with different boundaries
circular_db(center = c(5, 2), radius = 1.5, iris_data = iris_subset)

circular_db(center = c(5.5, 2), radius = 0.5, iris_data = iris_subset)

# Improve accutracy with manual inspection
circular_db(center = c(6, 2), radius = 1, iris_data = iris_subset)

```

The final circle achieves the best accuracy. The performance could be further improved
by careful adjujstment of the decision boundary, but as with the linear classifier,
a perfect model is not possible with this data. 

# Objective Function

## A. Mean Squared Error 

The next task is to write a function which calculates the mean-squared error for a given 
decision boundary. To calculate a mean-squared error, I will need to replace the threshold
activation with a sigmoid function that outputs class probabilities. The mean-squared 
error is then computed as the average of the squared residuals of the predictions.
The residuals are the difference between the correct answer and the predicted
output. The objective is to minimize the mean squared error by selecting
the best decision boundary. 

```{r}
# Sigmoid function
sigmoid <- function(x) {return(1 / (1 + exp(-x)) )}

# Takes in data with correct values (labels) and weights of decision boundary
# Weights should be in form (w0, w1, w2) and labels are 0 (versicolor) 
# or 1 (virginica).  
create_model <- function(iris_data, weights, plot_results = TRUE) {
  
  iris_data$intercept <- 1
  
  # Enforce column order for features and create label vector
  data <- dplyr::select(iris_data, intercept, petal_length, petal_width)
  labels <- ifelse(iris_data$species == 'versicolor', 0, 1)
  
  predictions <- c()
  
  # Make predictions on all of the data
  for (i in 1:nrow(data)) {
    prediction <- sigmoid(as.numeric(data[i, ]) %*% weights)[[1]]
    predictions <- c(predictions, prediction)
  }
  
  # Calculate the mean squared error
  mse <- mean( (predictions - labels) ^ 2)
  
  if (plot_results) {
  print(ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) + 
    geom_jitter() + 
      geom_abline(slope = -weights[2]/weights[3], intercept = -weights[1]/weights[3], 
                  color = 'blue', lwd = 1.1) + 
    xlab('Petal Length (cm)') + 
  ylab('Petal Width (cm)') + 
  ggtitle(sprintf('w0: %0.2f, w1: %0.2f, w2: %0.2f', weights[1], weights[2], weights[3]))
  + theme_classic(12) + 
      scale_color_manual(values = c('firebrick', 'darkgreen')))
  }
  
  print(sprintf('MSE on iris data set: %0.4f', mse))
}

# Example application
weights <- c(-4, 0.5, 1)

create_model(iris_subset, weights)
```
## B. Plot a decision boundary with a high and low MSE

The low MSE corresponds to the linear decision boundary previously drawn that 
was determined by repeated testing. 
A high MSE can also be found by some experimentation. 

```{r}
# Low MSE
create_model(iris_subset, weights = c(-4.6, 0.6, 1.1))

# High MSE 
create_model(iris_subset, weights = c(-4.5, 0.9, 1.1))
```
## C. Derivation of MSE with respect to weights

Class probabilities of an observation are made with the following equation:

$$ p_n = \sigma (w_0  + w_1 * length_n + w_2 * width_n)$$ 
Where $\sigma$ is the sigmoid activation function. The mean squared error
is therefore 

$$mse = \frac{1}{N} \sum\limits_{n=1}^{N} ( \sigma(w_0 + w_1 * L_n + w_2 * W_n) - y_n)^2 $$ 
Where $y_n$ is the known label for the observation. M refers to the total number
of observations (data points) which in this problem is 100. The sigmoid activation function
has the nice feature that the derivative is:

$$\frac{d\sigma(x)}{dx} = \sigma(x) * (1 - \sigma(x))$$ 
To simply the equation, we can define the weight row vector as 
$$\textbf{w} = [w_o, w_1, w_2]$$ 
and all of the observations as a matrix

$$\textbf{x} = \left( \begin{array}{ccc}
1 & L_1 & W_1 \\
1 & L_2 & W_2 \\
\vdots & \vdots & \vdots\\
1 & L_m & W_m \end{array} \right)$$ 

The labels are a column vector 

$$\textbf{y} = \begin{bmatrix}
           y_{1} \\
           y_{2} \\
           \vdots \\
           y_{n}
         \end{bmatrix}$$
         
The MSE is now expressed as

$$mse = \frac{1}{N}  ( \sigma(\textbf{x} * \textbf{w}) - \textbf{y})^2$$
The gradient of the mse with respect to the weights is

$$\frac{\partial(mse)}{d\textbf{w}} = \frac{2}{N} * \textbf{x} * (1 - \sigma(\textbf{x} * \textbf{w}) * ( \sigma(\textbf{x} * \textbf{w}) - \textbf{y}) $$
If we substitute in the probability already calculated, the equation becomes:

$$\frac{\partial(mse)}{d\textbf{w}} = \frac{2}{N} * \textbf{x} * (1 - \textbf{p}) * (\textbf{p} - \textbf{y}) $$

## D. Scalar versus Vector Gradient

The full scalar gradient for weight $w_i$ is 

$$\frac{\partial(mse)}{\partial{w_i}} = \frac{2}{N} * \sum\limits_{n=1}^{N} ( x_{i,n} *  (1 - \sigma(\textbf{x}_n * \textbf{w})*(\sigma(\textbf{x}_n * \textbf{w}) - y_n)) $$
In this equation, n is the observation, and i is the weight. In this problem,
there are three weights and 100 observations, so for each iteration, we would need to 
perform a sum over all 100 observations 3 separate times to update the three weights.

The computation requirements can be greatly reduced by expressing the observations 
as a (100 by 3) matrix, the weights as a (3 by 1) vector, and the labels as a 
(100 by 1) vector. The gradient of the mean squared error with respect to the weights:

$$\frac{\partial(mse)}{\partial\textbf{w}} = \frac{2}{N}  * (1 - \sigma(\textbf{x} * \textbf{w})) * (\sigma(\textbf{x} * \textbf{w}) - \textbf{y}) * \textbf{x}$$

The gradient update rule updates the weights on each iteration

$$\textbf{w}_{t+1} = \textbf{w}_t - \epsilon * \frac{\partial(mse)}{\partial(\textbf{w}_t)}$$ 
## E. Calculate the Gradient and Update the Weights

The algorithm is relatively straightforward to implement. I can initialize the 
weights, make the predictions, calculate the gradient, update the weights, and repeat
the process with the updated weights. 

```{r}
# Function takes in the iris data set, initial weights, number of iterations, 
# and the learning rate
grad_descent <- function(iris_data, initial_weights, n_steps = 5, 
                         plot_decision = TRUE, plot_learning = FALSE,
                         learning_rate = 0.01) {
  
  # Add intercept to data and determine n
  iris_data$intercept <- 1
  n_obs <- nrow(iris_data)
  
  # Enforce column order for features and create label vector
  data <- dplyr::select(iris_data, intercept, petal_length, petal_width)
  labels <- ifelse(iris_data$species == 'versicolor', 0, 1)
  
  weights <- initial_weights
  data <- data.matrix(data)
  
  # Variables to hold metrics
  results_df <- c()
  all_mse <- c()
  
  for (i in 1:n_steps) {
    
    # Make predictions and calculate errors
    predictions <- sapply(data %*% weights, sigmoid)
    errors <- predictions - labels
   
    # Record mean squared error
    mse <- mean((errors) ^ 2)
    all_mse <- c(all_mse, mse)
    
    # Calculate the gradient
    gradient <- (2 / n_obs) * matrix((1 - predictions) * (errors), ncol = n_obs) %*% data
    
    # Update the weights
    weights <- as.numeric(weights - learning_rate * gradient)
    
    print(sprintf('Iteration: %0.0f, mse: %0.4f', i, mse))
    
    # Plot the decision boundary if specified
    if (plot_decision) {
    p1 <- ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
      geom_jitter() +
        geom_abline(slope = -weights[2]/weights[3], intercept = -weights[1]/weights[3],
                    color = 'blue', lwd = 1.1) +
      xlab('Petal Length (cm)') +
    ylab('Petal Width (cm)') +
    ggtitle(sprintf('Iteration: %0.0f MSE: %0.4f    w0: %0.2f, w1: %0.2f, w2: %0.2f', 
                    i, mse, weights[1], weights[2], weights[3])) + 
      theme_classic(12) + coord_cartesian(xlim = c(2.5, 8), ylim = c(1.0, 4.0)) + 
        scale_color_manual(values = c('firebrick', 'darkgreen'))
    
    print(p1)
    }
  }
  
  results_df$mse <- all_mse
  results_df$iteration <- 1:n_steps
    
    # Plot the learning curve if specified
    if (plot_learning) {
      p2 <- ggplot(results_df, aes(x = iteration, y = all_mse)) + 
        geom_point(size = 1.2, col = 'darkgreen', shape = 2) + 
        geom_line(lwd = 1.2, col = 'darkgreen') + ylab('Mean Squared Error') + 
        ggtitle(sprintf('Learning Curve with step: %0.4f', learning_rate)) + 
        theme_classic(12)
    }
  
}
  
grad_descent(iris_subset, initial_weights = c(-6, 0.5, 1))  

grad_descent(iris_subset, initial_weights = c(-3.5, 0.5, 1))

```

As can be seen, the algorithm successfully updates the decision boundary
to decrease the mean squared error with each iteration. If the decision boundary
starts off too low, it is raised higher, and when it starts off too high,
it is slowly lower. With enough steps and a small enough learning rate, 
the algorithm will converge on the optimal decision boundary. 

# Optimization Using Gradient Descent

## A. Implement Gradient Descent

The function above already implements gradient descent on the iris data set. 
To perform the algorithm, the function needs a set of initial weights (w0, w1, w2), 
a learning rate (the default is 0.01), a number of iterations (the default is 5). 
I need to modify the function to plot the learning curve over time. 

## B. Plot the decision boundary and learning curve

```{r}
# Function takes in the iris data set, initial weights, number of iterations, 
# learning rate, and tolerance
# Return is a dataframe that can be used for plotting
# the decision boundary and learning curve
grad_descent_complete <- function(iris_data, initial_weights, n_steps = 5, 
                         learning_rate = 0.01, tolerance = 0.005) {
  
  # Add intercept to data and determine n
  iris_data$intercept <- 1
  n_obs <- nrow(iris_data)
  
  # Enforce column order for features and create label vector
  data <- dplyr::select(iris_data, intercept, petal_length, petal_width)
  labels <- ifelse(iris_data$species == 'versicolor', 0, 1)
  
  weights <- initial_weights
  data <- data.matrix(data)
  
  # Variables to hold metrics
  results_df <- as.data.frame(matrix(ncol = 5, nrow = n_steps))
  names(results_df) <- c('iteration', 'mse', 'w0', 'w1', 'w2')
  
  # Iterate for the specified number of steps
  for (i in 1:n_steps) {
    
    # Make predictions and calculate errors
    predictions <- sapply(data %*% weights, sigmoid)
    errors <- predictions - labels
   
    # Record mean squared error
    mse <- mean((errors) ^ 2)
    
    # Calculate the gradient
    gradient <- (2 / n_obs) * matrix((1 - predictions) * (errors), ncol = n_obs) %*% data

    # Record results for plotting
    results_df[i, 'iteration'] = i
    results_df[i, 'mse'] = mse
    results_df[i, 'w0'] = weights[1]
    results_df[i, 'w1'] = weights[2]
    results_df[i, 'w2'] = weights[3]
    
    
    # First stopping criteria
    if (sqrt(sum(gradient ^ 2)) < tolerance) {
      print("Minimum Tolerance Reached.")
      return(results_df)
    }
    
    # Second stopping criteria
    if (mse > min(results_df$mse, na.rm = TRUE)) {
      print('MSE Inceasing.')
      return(results_df)
    }
    
    # Update the weights
    weights <- as.numeric(weights - learning_rate * gradient)
    
    # Display progress every 50 iterations
    if (i %% 50 == 0) {print(sprintf('Iteration: %0.0f, mse: %0.4f', i, mse))
      
    }
  }
  # Max iterations reached
  print('Maximum number of iterations reached.')
  return(results_df)
}
```

## C. Show results from the iris data set

I will show two different results of the algorithm, one starting with 
an initally too high decision boundary, and the other with an initially too low
decision boundary. The algorithms are run until the maximum number of steps 
is reached, or thesquare of the sum of the gradients is below the specified
olerance. The results can then be plotted to visualize the change in decision 
boundary and the learning curve. 

```{r}
# Start with a high decision boundary
results_high <- grad_descent_complete(iris_subset, initial_weights = c(-6, 0.4, 1.2), 
                      n_steps = 500, learning_rate = 0.005, tolerance = 0.005)

# Start with a low decision boundary
results_low <- grad_descent_complete(iris_subset, initial_weights = c(-3.6, 0.6, 1.2), 
                      n_steps = 500, learning_rate = 0.005, tolerance = 0.005)

```

The model that started with the high decision boundary continued to improve until
the maxmium number of iterations was reached while the decision boundary that started
too low stopped after a number of itertions when the mse started to increase,
indicating the algorithm was jumping around the minimum. Gradient descent is only
guaranteed to find a minimum if the step size is small enough to actually reach the 
minimum, otherwise it can bounce around the minimum and diverge. This is one
reason to use a step size that decreases the number of iterations. 

I can now plot the learning curves and decision boundary changes for 
both runs of the model. 

```{r}
# Function to plot the decision boundary and the learning curve
plot_decision_learning <- function(results, iris_data) {
  
  # Plot results from beginning, middle, and end of iterations
  rows <- seq(1, nrow(results), by = nrow(results)/2 - 1) 
  
  for  (i in rows) {
    # Select the data for plotting
    data_row <- results[i, ]
    weights <- c(data_row$w0, data_row$w1, data_row$w2)
    i <- data_row$iteration
    mse <- data_row$mse
  
  # Plot decision boudary
  print(ggplot(iris_data, aes(x = petal_length, y = petal_width, color = species)) +
      geom_jitter() +
        geom_abline(slope = -weights[2]/weights[3], 
                    intercept = -weights[1]/weights[3],
                    color = 'blue', lwd = 1.1) +
      xlab('Petal Length (cm)') +
    ylab('Petal Width (cm)') +
    ggtitle(sprintf('Iteration: %0.0f MSE: %0.4f    w0: %0.2f, w1: %0.2f, w2: %0.2f', 
                    i, mse, weights[1], weights[2], weights[3])) + 
      theme_classic(12) + coord_cartesian(xlim = c(2.5, 8), ylim = c(1.0, 4.0)) + 
        scale_color_manual(values = c('firebrick', 'darkgreen')))
  
  learning_data <- results[1:i, ]
  print(ggplot(learning_data, aes(x = iteration, y = mse)) + 
    geom_point(color = 'firebrick', shape = 4) +  ggtitle("Learning curve"))
  }
  
  
  
}

# Plot results for first run
results_high <- results_high[complete.cases(results_high), ]
plot_decision_learning(results_high, iris_subset)

# Plot results for second run
results_low <- results_low[complete.cases(results_low), ]
plot_decision_learning(results_low, iris_subset)

```



## D. How to Choose a learning step

The learning rate was selected by trial and error. When choosing an error rate, 
one can make the error of choosing too large, or too small. Too great of a learning
rate and the algorithm will never converge but "jump" around the minimum. Too tiny of a learning
rate and the algorithm will converge, but it will take a long time to do so. 
I experimented with a number of different learning rates and eventually decided on
0.005. This seemed to converge in a relatively short number of iterations and did
not diverge from the minimum mse. If this were a tougher problem, I would 
use a learning rate schedule where the learning rate decreases with the number of
iterations. However, this was not required for such a simple classification problem.
Two common learning rate schedules are as follows where $\epsilon_0$ 
is the initial learning rate. 

$$\epsilon = \epsilon_0 / t$$
This is a linear decrease in the learning rate with the number of iterations

$$\epsilon = \epsilon_0 * e^{-t}$$ 
This is an exponential decrease in the learning rate with the number of iterations.

The benefit of a learning rate schedule is that one can begin a problem with a higher
learning rate to quickly reach the vicinity of the minimum, and then the learning rate 
decreases with the number of iterations in order to converge to the absolute minimum. 
Choosing the correct learning rate schedule can make a significant 
difference in the runtime and accuracy of a model. 


## E. How to Choose a stopping criteria

I instituted two different checks for the stopping criteria. The first was based on
a tolerance threshold for the minimum change in the gradients. If the gradients 
are not changing above a certain threshold (set at 0.001), then the algorithm
should stop. I used the square root of the sum of the gradients squared to measure
the magnitude of the gradients. This stopping criterion is effective because the 
magnitude of the gradients is directly proportional to the error, and therefore, 
when the gradients decrease below the threshold, that means the error is not decreasing
rapidly anymore and further iterations are unnecessary.

The second stopping measure was if the mean-squared error began to increase. 
This can occur if the algorith misses the absolute minimum and begins to 
bounce around the true minimum. The cause of this would be a learning rate that 
is too great. When running the model, the first time the algorithm was stopped 
bythe maximum number of iterations, and the second time was stopped by the 
mean squared error increasing. Overall, the algorithm should 
be able to find a value for the mse within a small distance of the absolute minimum
depending on the step size. The algorithm developed in this report was able to 
find an optimal decision boundary for the iris classification task to the extent 
that a linear classifier can separate the two species. 